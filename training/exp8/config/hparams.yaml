
### Training data paths
train_path: "/mnt/yandex_cup24/CoverHunterMPS_data/covers80/train.txt"
# val_path: "/mnt/yandex_cup24/CoverHunterMPS_data/covers80/val.txt"
# test_path: "/mnt/yandex_cup24/CoverHunterMPS_data/covers80/val.txt"
# test_path: "data/covers80/test.txt"

## optional external test datasets used in the test phase of training epochs
covers80:
  query_path: "/mnt/yandex_cup24/CoverHunterMPS_data/covers80/query.txt"
  ref_path: "/mnt/yandex_cup24/CoverHunterMPS_data/covers80/ref.txt"
  every_n_epoch_to_test: 1  # validate after every n epoch

# Download the reels50easy dataset from https://www.irishtune.info/public/MLdata.htm
# and then you can uncomment the next 4 lines.
#reels50easy:
#  query_path: "data/reels50easy_testset/full.txt"  # path to your bench
#  ref_path: "data/reels50easy_testset/full.txt"
#  every_n_epoch_to_test: 1

# Download the reels50hard dataset from https://www.irishtune.info/public/MLdata.htm
# and then you can uncomment the next 4 lines.
#reels50hard:
#  query_path: "data/reels50hard_testset/full.txt"  # path to your bench
#  ref_path: "data/reels50hard_testset/full.txt"
#  every_n_epoch_to_test: 1

#shs_test:
#  query_path: "training/shs100k/test.txt"
#  ref_path: "training/shs100k/test.txt"
#  every_n_epoch_to_test: 2 # validate after every n epoch

#da-tacos: (see https://github.com/MTG/da-tacos)
#  query_path: "training/benchmark_query.txt"
#  ref_path: "training/benchmark_ref.txt"
#  query_in_ref_path: "training/datacos/query_in_ref.txt"
#  every_n_epoch_to_test: 2  # validate after every n epoch

every_n_epoch_to_test: 1  # validate after every n epoch
every_n_epoch_to_save: 1  # save model after every n epoch

### Dataset parameters
data_type: "cqt"  # raw or cqt or mel
chunk_frame: [50]
chunk_s: 60 # = chunk_frame[0] / 25 * mean_size
mode: "random"
mean_size: 1
m_per_class: 32
cqt:
  # hop_size: 0.04  # 1s has 25 frames
  hop_size: 1.2     # 1s has 25 frames

spec_augmentation:
  # random_erase:
  #   prob: 0.2
  #   erase_num: 1
  #   region_size: [0.05, 1.0] # w, h as fraction of feature array size
  roll_pitch:
    prob: 0.3
    shift_num: 2
    method: "default" # "default" "low_melody" or "flex_melody"

### Training parameters
device: 'cuda' # 'mps' or 'cuda' 
seed: 1234
num_workers: 1
num_gpus: 1
batch_size: 3200  # 256. CoverHunter repo started with 16, but that is too small.
learning_rate: 0.001
first_cycle_steps: 8000
weight_decay: 0.01
adam_b1: 0.8
adam_b2: 0.99
lr_decay: 0.96  # CoverHunter used 0.9975 but that's for very large datasets
min_lr: 0.00001
warmup: False
warmup_steps: 20000
early_stopping_patience: 6

### Model parameters
input_dim: 84 # default 96 frequency bins
embed_dim: 128
encoder:  # model-encode
  output_dims: 128
  num_blocks: 6
  attention_dim: 256
  input_layer: "linear"

pool_type: "attention"

# loss parameters
ce:  # cross-entropy
  output_dims: 41616
  weight: 1.0
  gamma: 2

triplet:
  margin: 0.3
  weight: 0.3

center:
  weight: 0.0
